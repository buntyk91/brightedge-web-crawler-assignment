# ðŸŒ Core Web Crawler

A scalable, production-grade web crawler built in Java (Spring Boot) that crawls, parses, and stores structured metadata from any URL. Supports distributed crawling, fault tolerance, robots.txt respect, and dual storage (MySQL + MongoDB). Comes with a Streamlit-based UI for triggering crawls and viewing results.

---

## âœ¨ Features

- ðŸ” **Batch URL ingestion** from MySQL
- ðŸŒ **Crawls any public URL** (HTML pages)
- âš™ï¸ **Retry & Dead Letter Queue** using Kafka
- ðŸ§  **Smart Parser** extracts title, description, text, meta tags, OpenGraph, schema.org, forms, images, links, etc.
- ðŸ›¢ï¸ **Stores metadata**
  - Structured: MySQL
  - Raw + extended: MongoDB
- ðŸš« **Respects robots.txt**
- ðŸ“œ **Schema-independent dumping** of all HTML metadata
- ðŸ“Š **Streamlit UI** to run crawler and see results
- â˜ï¸ **Hosted on AWS EC2** for public access

---

## ðŸ§± Architecture Overview

```plaintext
MySQL (seed URLs) --> Spring Boot REST API --> Crawler
                             |                     |
                        Kafka (DLQ)            HTML Parser
                             |                     |
                          Streamlit UI        MongoDB + MySQL
